{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Gradient Descent\n",
    "This Multilayer Perceptron and the Backpropagation are still a part of Gradient Descent in Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:17:25.920461Z",
     "iopub.status.busy": "2020-11-12T20:17:25.920461Z",
     "iopub.status.idle": "2020-11-12T20:17:25.940461Z",
     "shell.execute_reply": "2020-11-12T20:17:25.940461Z",
     "shell.execute_reply.started": "2020-11-12T20:17:25.920461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's have a look at how the np.dot() thing works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each perceptron (3 in total) receives every input (there are 4 inputs). So we need for weights per perceptron (one for every input to the perceptron). Each column in the matrix is one perceptron of the (first and only) hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:19:54.630461Z",
     "iopub.status.busy": "2020-11-12T20:19:54.630461Z",
     "iopub.status.idle": "2020-11-12T20:19:54.640461Z",
     "shell.execute_reply": "2020-11-12T20:19:54.640461Z",
     "shell.execute_reply.started": "2020-11-12T20:19:54.630461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02341534, -0.0234137 ,  0.15792128],\n",
       "       [ 0.07674347, -0.04694744,  0.054256  ],\n",
       "       [-0.04634177, -0.04657298,  0.02419623],\n",
       "       [-0.19132802, -0.17249178, -0.05622875]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_input_to_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:26:06.148749Z",
     "iopub.status.busy": "2020-11-12T20:26:06.148749Z",
     "iopub.status.idle": "2020-11-12T20:26:06.168771Z",
     "shell.execute_reply": "2020-11-12T20:26:06.168771Z",
     "shell.execute_reply.started": "2020-11-12T20:26:06.148749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_input_to_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:20:03.670461Z",
     "iopub.status.busy": "2020-11-12T20:20:03.670461Z",
     "iopub.status.idle": "2020-11-12T20:20:03.680461Z",
     "shell.execute_reply": "2020-11-12T20:20:03.680461Z",
     "shell.execute_reply.started": "2020-11-12T20:20:03.670461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:25:55.181589Z",
     "iopub.status.busy": "2020-11-12T20:25:55.181589Z",
     "iopub.status.idle": "2020-11-12T20:25:55.201611Z",
     "shell.execute_reply": "2020-11-12T20:25:55.201611Z",
     "shell.execute_reply.started": "2020-11-12T20:25:55.181589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:19:21.900461Z",
     "iopub.status.busy": "2020-11-12T20:19:21.900461Z",
     "iopub.status.idle": "2020-11-12T20:19:21.920461Z",
     "shell.execute_reply": "2020-11-12T20:19:21.920461Z",
     "shell.execute_reply.started": "2020-11-12T20:19:21.900461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415],\n",
       "       [-0.1382643 ],\n",
       "       [ 0.64768854],\n",
       "       [ 1.52302986]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:26:27.591525Z",
     "iopub.status.busy": "2020-11-12T20:26:27.591525Z",
     "iopub.status.idle": "2020-11-12T20:26:27.611547Z",
     "shell.execute_reply": "2020-11-12T20:26:27.611547Z",
     "shell.execute_reply.started": "2020-11-12T20:26:27.591525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signals arrive at the perceptrons. They are modulated by the weights and then summed up. This is what the numpy dot product of two arrays is for (np.dot()).\n",
    "\n",
    "But let's have a closer look of what is happening under the hood. We multiply the weights matrix with a column vector. So far our features are only present as a row vector. We can manually translate them to a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:32:15.394316Z",
     "iopub.status.busy": "2020-11-12T20:32:15.394316Z",
     "iopub.status.idle": "2020-11-12T20:32:15.414324Z",
     "shell.execute_reply": "2020-11-12T20:32:15.414324Z",
     "shell.execute_reply.started": "2020-11-12T20:32:15.394316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01163073, -0.01162991,  0.07844174],\n",
       "       [-0.01061088,  0.00649115, -0.00750167],\n",
       "       [-0.03001503, -0.03016478,  0.01567162],\n",
       "       [-0.29139829, -0.26271014, -0.08563807]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_X = weights_input_to_hidden * X[:,None]\n",
    "weighted_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we sum up the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:32:16.694836Z",
     "iopub.status.busy": "2020-11-12T20:32:16.694836Z",
     "iopub.status.idle": "2020-11-12T20:32:16.704840Z",
     "shell.execute_reply": "2020-11-12T20:32:16.704840Z",
     "shell.execute_reply.started": "2020-11-12T20:32:16.694836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34365494, -0.29801368,  0.00097362])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_X.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done more easily with the numpy dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:20:28.730461Z",
     "iopub.status.busy": "2020-11-12T20:20:28.730461Z",
     "iopub.status.idle": "2020-11-12T20:20:28.740461Z",
     "shell.execute_reply": "2020-11-12T20:20:28.740461Z",
     "shell.execute_reply.started": "2020-11-12T20:20:28.730461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34365494, -0.29801368,  0.00097362])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron with Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:01:45.354461Z",
     "iopub.status.busy": "2020-11-12T20:01:45.344461Z",
     "iopub.status.idle": "2020-11-12T20:01:46.594461Z",
     "shell.execute_reply": "2020-11-12T20:01:46.594461Z",
     "shell.execute_reply.started": "2020-11-12T20:01:45.354461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(weights_hidden_output, output_error_term) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full implementation of a Neural Network with Forward Pass and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T20:10:23.889461Z",
     "iopub.status.busy": "2020-11-12T20:10:23.888461Z",
     "iopub.status.idle": "2020-11-12T20:10:23.909461Z",
     "shell.execute_reply": "2020-11-12T20:10:23.908461Z",
     "shell.execute_reply.started": "2020-11-12T20:10:23.889461Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('../data/backpropagation.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(weights_input_hidden, features)\n",
    "        hidden_output = None\n",
    "        output = None\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = None\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = None\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = None\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = None\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += 0\n",
    "        del_w_input_hidden += 0\n",
    "\n",
    "    # TODO: Update weights  (don't forget to division by n_records or number of samples)\n",
    "    weights_input_hidden += 0\n",
    "    weights_hidden_output += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
